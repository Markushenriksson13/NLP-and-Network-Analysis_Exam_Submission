{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breaking Bad Data Processing with LLMs ðŸ–Šï¸ âš™ï¸ ðŸ“¨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook leverages Large Language Models (LLMs) - to fetch the dialogue (subtitles) of the TV series *Breaking Bad* from the Movie and TV-show wiki: *Fandom.com* and extract relationships between characters, locations, events & season number.\n",
    "\n",
    "#### 1. Data Acquisition\n",
    "\n",
    "- **Scraping**: Subtitles are scraped from the Breaking Bad Fandom Wiki for all seasons and saved as individual text files.\n",
    "- **Cleaning**: The scraped subtitles are cleaned to remove timestamps and other unnecessary elements, leaving only the dialogue.\n",
    "\n",
    "#### 2. Context and Prompt Creation\n",
    "\n",
    "- **Wikipedia Summary**: The notebook utilizes the Wikipedia article \"List of characters in the Breaking Bad franchise\" to create a summarized context of key characters and their relationships using the LLM: `meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo`. This summary is used as background knowledge for the LLM processing of the subtitles (dialogue in the TV-show).\n",
    "- **LLM Prompts**: System prompts are  defined for the LLM. These prompts guide its analysis, ensuring that the extracted information follows a predefined JSON schema for representing relationships.\n",
    "\n",
    "#### 3. LLM Processing, Extraction & Output\n",
    "\n",
    "- **Episode Analysis**: The LLM (`Qwen/Qwen2.5-72B-Instruct-Turbo`) iterates through each episode's subtitle file. The content of the subtitles, along with details like episode name and season number, are fed to the LLM as prompts.\n",
    "- **Entity and Relationship Extraction**: The LLM analyzes the script and extracts entities (characters, locations, events). It then identifies relationships between these entities using a set of predefined relationship types (e.g., \"friend of,\" \"enemy of,\" \"works with\").\n",
    "- **JSON Structuring**: The extracted information is structured into a JSON format, for easy storage and further analysis...\n",
    "\n",
    "- **JSON Output**: All the LLM-processed episode data is saved into a single JSON file named \"breaking_bad_analysisV2.json\".\n",
    "- **Summary**: A summary indicating the number of processed episodes is displayed to the user along with failed episodes (if it unlikely would occur)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_hvW0WOLCxl"
   },
   "source": [
    "### Install & Import Libraries ðŸŽ›ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UX4ZBS_FV99u",
    "outputId": "7664b6a4-c2a9-4ec5-aeb9-19400add0bcb"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n  var py_version = '3.4.1'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  var reloading = false;\n  var Bokeh = root.Bokeh;\n\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      run_callbacks();\n      return null;\n    }\n    if (!reloading) {\n      console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error() {\n      console.error(\"failed to load \" + url);\n    }\n\n    var skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {}, 'shim': {}});\n      root._bokeh_is_loading = css_urls.length + 0;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    var existing_stylesheets = []\n    var links = document.getElementsByTagName('link')\n    for (var i = 0; i < links.length; i++) {\n      var link = links[i]\n      if (link.href != null) {\n\texisting_stylesheets.push(link.href)\n      }\n    }\n    for (var i = 0; i < css_urls.length; i++) {\n      var url = css_urls[i];\n      if (existing_stylesheets.indexOf(url) !== -1) {\n\ton_load()\n\tcontinue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    var existing_scripts = []\n    var scripts = document.getElementsByTagName('script')\n    for (var i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n\texisting_scripts.push(script.src)\n      }\n    }\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (var i = 0; i < js_modules.length; i++) {\n      var url = js_modules[i];\n      if (skip.indexOf(url) !== -1 || existing_scripts.indexOf(url) !== -1) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      var url = js_exports[name];\n      if (skip.indexOf(url) >= 0 || root[name] != null) {\n\tif (!window.requirejs) {\n\t  on_load();\n\t}\n\tcontinue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  var js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.4.1.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.4.1.min.js\", \"https://cdn.holoviz.org/panel/1.4.4/dist/panel.min.js\"];\n  var js_modules = [];\n  var js_exports = {};\n  var css_urls = [];\n  var inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n\ttry {\n          inline_js[i].call(root, root.Bokeh);\n\t} catch(e) {\n\t  if (!reloading) {\n\t    throw e;\n\t  }\n\t}\n      }\n      // Cache old bokeh versions\n      if (Bokeh != undefined && !reloading) {\n\tvar NewBokeh = root.Bokeh;\n\tif (Bokeh.versions === undefined) {\n\t  Bokeh.versions = new Map();\n\t}\n\tif (NewBokeh.version !== Bokeh.version) {\n\t  Bokeh.versions.set(NewBokeh.version, NewBokeh)\n\t}\n\troot.Bokeh = Bokeh;\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      root._bokeh_is_initializing = true\n      root._bokeh_onload_callbacks = []\n      var bokeh_loaded = Bokeh != null && (Bokeh.version === py_version || (Bokeh.versions !== undefined && Bokeh.versions.has(py_version)));\n      if (!reloading && !bokeh_loaded) {\n\troot.Bokeh = undefined;\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n\tconsole.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n\trun_inline_js();\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        }) \n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>*[data-root-id],\n",
       "*[data-root-id] > * {\n",
       "  box-sizing: border-box;\n",
       "  font-family: var(--jp-ui-font-family);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
       "}\n",
       "\n",
       "/* Override VSCode background color */\n",
       ".cell-output-ipywidget-background:has(\n",
       "    > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
       "  ),\n",
       ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
       "  background-color: transparent !important;\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='p1002'>\n",
       "  <div id=\"fb3f816c-5af2-43ca-b188-a0248dafd32d\" data-root-id=\"p1002\" style=\"display: contents;\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  var docs_json = {\"2e5d5424-f5d6-439e-b864-a998ba3f2248\":{\"version\":\"3.4.1\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"panel.models.browser.BrowserInfo\",\"id\":\"p1002\"},{\"type\":\"object\",\"name\":\"panel.models.comm_manager.CommManager\",\"id\":\"p1003\",\"attributes\":{\"plot_id\":\"p1002\",\"comm_id\":\"2a313cd77a2747e998d8cf2fbb93aba5\",\"client_comm_id\":\"e7ef6e80183b427ebc938dadeefa4383\"}}],\"defs\":[{\"type\":\"model\",\"name\":\"ReactiveHTML1\"},{\"type\":\"model\",\"name\":\"FlexBox1\",\"properties\":[{\"name\":\"align_content\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"align_items\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"flex_direction\",\"kind\":\"Any\",\"default\":\"row\"},{\"name\":\"flex_wrap\",\"kind\":\"Any\",\"default\":\"wrap\"},{\"name\":\"gap\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"justify_content\",\"kind\":\"Any\",\"default\":\"flex-start\"}]},{\"type\":\"model\",\"name\":\"FloatPanel1\",\"properties\":[{\"name\":\"config\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"contained\",\"kind\":\"Any\",\"default\":true},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"right-top\"},{\"name\":\"offsetx\",\"kind\":\"Any\",\"default\":null},{\"name\":\"offsety\",\"kind\":\"Any\",\"default\":null},{\"name\":\"theme\",\"kind\":\"Any\",\"default\":\"primary\"},{\"name\":\"status\",\"kind\":\"Any\",\"default\":\"normalized\"}]},{\"type\":\"model\",\"name\":\"GridStack1\",\"properties\":[{\"name\":\"mode\",\"kind\":\"Any\",\"default\":\"warn\"},{\"name\":\"ncols\",\"kind\":\"Any\",\"default\":null},{\"name\":\"nrows\",\"kind\":\"Any\",\"default\":null},{\"name\":\"allow_resize\",\"kind\":\"Any\",\"default\":true},{\"name\":\"allow_drag\",\"kind\":\"Any\",\"default\":true},{\"name\":\"state\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"drag1\",\"properties\":[{\"name\":\"slider_width\",\"kind\":\"Any\",\"default\":5},{\"name\":\"slider_color\",\"kind\":\"Any\",\"default\":\"black\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":50}]},{\"type\":\"model\",\"name\":\"click1\",\"properties\":[{\"name\":\"terminal_output\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"debug_name\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"clears\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"FastWrapper1\",\"properties\":[{\"name\":\"object\",\"kind\":\"Any\",\"default\":null},{\"name\":\"style\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"NotificationAreaBase1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"NotificationArea1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"notifications\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0},{\"name\":\"types\",\"kind\":\"Any\",\"default\":[{\"type\":\"map\",\"entries\":[[\"type\",\"warning\"],[\"background\",\"#ffc107\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-exclamation-triangle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]},{\"type\":\"map\",\"entries\":[[\"type\",\"info\"],[\"background\",\"#007bff\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-info-circle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]}]}]},{\"type\":\"model\",\"name\":\"Notification\",\"properties\":[{\"name\":\"background\",\"kind\":\"Any\",\"default\":null},{\"name\":\"duration\",\"kind\":\"Any\",\"default\":3000},{\"name\":\"icon\",\"kind\":\"Any\",\"default\":null},{\"name\":\"message\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"notification_type\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_destroyed\",\"kind\":\"Any\",\"default\":false}]},{\"type\":\"model\",\"name\":\"TemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"BootstrapTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"TemplateEditor1\",\"properties\":[{\"name\":\"layout\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"MaterialTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"copy_to_clipboard1\",\"properties\":[{\"name\":\"fill\",\"kind\":\"Any\",\"default\":\"none\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":null}]}]}};\n",
       "  var render_items = [{\"docid\":\"2e5d5424-f5d6-439e-b864-a998ba3f2248\",\"roots\":{\"p1002\":\"fb3f816c-5af2-43ca-b188-a0248dafd32d\"},\"root_ids\":[\"p1002\"]}];\n",
       "  var docs = Object.values(docs_json)\n",
       "  if (!docs) {\n",
       "    return\n",
       "  }\n",
       "  const py_version = docs[0].version.replace('rc', '-rc.').replace('.dev', '-dev.')\n",
       "  async function embed_document(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    await Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "    for (const render_item of render_items) {\n",
       "      for (const root_id of render_item.root_ids) {\n",
       "\tconst id_el = document.getElementById(root_id)\n",
       "\tif (id_el.children.length && id_el.children[0].hasAttribute('data-root-id')) {\n",
       "\t  const root_el = id_el.children[0]\n",
       "\t  root_el.id = root_el.id + '-rendered'\n",
       "\t  for (const child of root_el.children) {\n",
       "            // Ensure JupyterLab does not capture keyboard shortcuts\n",
       "            // see: https://jupyterlab.readthedocs.io/en/4.1.x/extension/notebook.html#keyboard-interaction-model\n",
       "\t    child.setAttribute('data-lm-suppress-shortcuts', 'true')\n",
       "\t  }\n",
       "\t}\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  function get_bokeh(root) {\n",
       "    if (root.Bokeh === undefined) {\n",
       "      return null\n",
       "    } else if (root.Bokeh.version !== py_version) {\n",
       "      if (root.Bokeh.versions === undefined || !root.Bokeh.versions.has(py_version)) {\n",
       "\treturn null\n",
       "      }\n",
       "      return root.Bokeh.versions.get(py_version);\n",
       "    } else if (root.Bokeh.version === py_version) {\n",
       "      return root.Bokeh\n",
       "    }\n",
       "    return null\n",
       "  }\n",
       "  function is_loaded(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    return (Bokeh != null && Bokeh.Panel !== undefined)\n",
       "  }\n",
       "  if (is_loaded(root)) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (is_loaded(root)) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 200) {\n",
       "          clearInterval(timer);\n",
       "\t  var Bokeh = get_bokeh(root)\n",
       "\t  if (Bokeh == null || Bokeh.Panel == null) {\n",
       "            console.warn(\"Panel: ERROR: Unable to run Panel code because Bokeh or Panel library is missing\");\n",
       "\t  } else {\n",
       "\t    console.warn(\"Panel: WARNING: Attempting to render but not all required libraries could be resolved.\")\n",
       "\t    embed_document(root)\n",
       "\t  }\n",
       "        }\n",
       "      }\n",
       "    }, 25, root)\n",
       "  }\n",
       "})(window);</script>"
      ]
     },
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "p1002"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Install required packages from requirements.txt\n",
    "!pip install -r https://raw.githubusercontent.com/Markushenriksson13/NLP-and-Network-Analysis_Exam_Submission/refs/heads/main/requirements.txt -q\n",
    "\n",
    "# importing the clear_output function from IPython.display module to reduce noise in outputs...\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Datascrapping libs\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Datahandling\n",
    "import os\n",
    "import re\n",
    "import splitfile\n",
    "\n",
    "# LLM Libs & Setup\n",
    "from openai import OpenAI\n",
    "import json\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "import textwrap\n",
    "\n",
    "# Wikipedia import\n",
    "import wikipediaapi\n",
    "\n",
    "# Network analysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "import networkx as nx\n",
    "import holoviews as hv\n",
    "import hvplot.networkx as hvnx\n",
    "from community import community_louvain\n",
    "from holoviews import opts\n",
    "import plotly.graph_objects as go\n",
    "import random\n",
    "from community import community_louvain\n",
    "\n",
    "# Model prediction\n",
    "from datasets import Dataset\n",
    "from setfit import SetFitModel, SetFitTrainer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Gradio deployment\n",
    "import gradio as gr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API SETUP (User Together API-Key Input required)... ðŸ“ ðŸ“ ðŸ“\n",
    "For the LLM-processing you will need to input a Together API Key in the field \"INSERT TOKEN\" below\n",
    " * (https://api.together.ai/signin)\n",
    "\n",
    " Please note that you can use Google Colab SECRETS, if you have saved your Together API Key there - \n",
    " * If yes? (Colab): \n",
    "    * 1. Remove the #'s below for Colab lib loading & TOGETHER_API_KEY = userdata.get('TOGETHER_API_KEY')\n",
    "    * 2. Remove the other line: TOGETHER_API_KEY = \"INSERT TOKEN\" or use #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup OpenAI client with custom TogetherAPI key and base URL\n",
    "\n",
    "# FOR COLAB YOU CAN USE GOOGLE COLAB SECRETS, if you have saved your Together API Key there - \n",
    "#  - If yes?: Remove the #'s below and remove the other line: TOGETHER_API_KEY = \"INSERT TOKEN\"\n",
    "\n",
    "# from google.colab import userdata\n",
    "# TOGETHER_API_KEY = userdata.get('TOGETHER_API_KEY')\n",
    "\n",
    "TOGETHER_API_KEY = \"INSERT TOKEN\" # INSERT YOUR TOKEN\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.together.xyz/v1\",\n",
    "    api_key=TOGETHER_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KbIw0o9ALCxn"
   },
   "source": [
    "### Setup + Data Extraction âš™ï¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "337iB-HZLCxo"
   },
   "source": [
    "#### Fetching Breaking Bad Data from Fandom (using subtitles of each season/episode) ðŸ“–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "17hWbYkCLCxp",
    "outputId": "4aba9819-3b8d-46df-c3d4-097b6ec14803"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manuscript Saved!\n"
     ]
    }
   ],
   "source": [
    "# Base URL\n",
    "base_url = \"https://breakingbad.fandom.com/wiki/Category:Breaking_Bad_Subtitles\"\n",
    "\n",
    "def get_season_links(base_url, target_seasons):\n",
    "    response = requests.get(base_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    season_links = []\n",
    "\n",
    "    for link in soup.select('a.category-page__member-link'):\n",
    "        for season in target_seasons:\n",
    "            if f\"Season_{season}\" in link['href']:\n",
    "                season_links.append(\"https://breakingbad.fandom.com\" + link['href'])\n",
    "    return season_links\n",
    "\n",
    "def get_episode_links(season_url):\n",
    "    response = requests.get(season_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    episode_links = []\n",
    "    for link in soup.select('a.category-page__member-link'):\n",
    "        episode_links.append(\"https://breakingbad.fandom.com\" + link['href'])\n",
    "    return episode_links\n",
    "\n",
    "def get_subtitles(episode_url):\n",
    "    response = requests.get(episode_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    subtitle_pre = soup.find(\"pre\")\n",
    "    if subtitle_pre:\n",
    "        subtitles = subtitle_pre.get_text(strip=True)\n",
    "        return subtitles\n",
    "    return \"\"\n",
    "\n",
    "def save_subtitles(episode_name, subtitles, season):\n",
    "    # Handle \"5A\" and \"5B\" cases\n",
    "    season_folder = f\"Season_{season}\"\n",
    "    os.makedirs(f\"subtitles/{season_folder}\", exist_ok=True)\n",
    "    file_path = f\"subtitles/{season_folder}/{season_folder} - {episode_name}.txt\"\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(subtitles)\n",
    "\n",
    "def scrape_and_save_subtitles():\n",
    "    target_seasons = [1, 2, 3, 4, \"5A\", \"5B\"] # ALL SEASONS\n",
    "    # REPLACE # IF ONLY SEASON 1 IS WANTED: target_seasons = [1] # ONLY SEASON 1?\n",
    "    season_links = get_season_links(base_url, target_seasons)\n",
    "\n",
    "    for season_url in season_links:\n",
    "        # Extract season from URL\n",
    "        season = None\n",
    "        for s in target_seasons:\n",
    "            if f\"Season_{s}\" in season_url:\n",
    "                season = s\n",
    "                break\n",
    "\n",
    "        if season:\n",
    "            episode_links = get_episode_links(season_url)\n",
    "            for episode_url in episode_links:\n",
    "                subtitles = get_subtitles(episode_url)\n",
    "                episode_name = episode_url.split(\"/\")[-1].replace(\"_\", \" \")\n",
    "                save_subtitles(episode_name, subtitles, season)\n",
    "                print(f\"Saved subtitles for {episode_name} in Season {season}\")\n",
    "# Run the scraper and saver\n",
    "scrape_and_save_subtitles()\n",
    "\n",
    "clear_output()\n",
    "print(\"Manuscript Saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CUUwsHp9Mbfk"
   },
   "source": [
    "#### Data Cleaning ðŸ§¹ ðŸ§¹ ðŸ§¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "_ktdrBJWMbMr"
   },
   "outputs": [],
   "source": [
    "def clean_subtitle(input_text):\n",
    "    # Fjern linjer med tidsstempler og numre\n",
    "    lines = input_text.split('\\n')\n",
    "    cleaned_lines = []\n",
    "    skip_next = False\n",
    "\n",
    "    for line in lines:\n",
    "        if skip_next:\n",
    "            skip_next = False\n",
    "            continue\n",
    "        if re.match(r'^\\d+$', line.strip()) or '-->' in line:\n",
    "            skip_next = True\n",
    "            continue\n",
    "        if line.strip():\n",
    "            cleaned_lines.append(line.strip())\n",
    "\n",
    "    return ' '.join(cleaned_lines)\n",
    "\n",
    "def process_directory(root_dir):\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.txt'):\n",
    "                file_path = os.path.join(dirpath, filename)\n",
    "\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read()\n",
    "\n",
    "                cleaned_content = clean_subtitle(content)\n",
    "\n",
    "                with open(file_path, 'w', encoding='utf-8') as file:\n",
    "                    file.write(cleaned_content)\n",
    "# path to the folder containing our fetched subtitles\n",
    "root_directory = 'subtitles/'\n",
    "\n",
    "process_directory(root_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8xRLQ1fqLCxp"
   },
   "source": [
    "### Definition of Extraction Schema ðŸ“¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JQlgke4TEnRL",
    "outputId": "2fcef5f5-8e85-4efd-83f8-8f0c5748a2b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:  List of characters in the Breaking Bad franchise\n",
      "Content:  Breaking Bad is a crime drama franchise created by American filmmaker Vince Gilligan. It started wit\n"
     ]
    }
   ],
   "source": [
    "# we initialize the wikipedia api\n",
    "wiki_wiki = wikipediaapi.Wikipedia(\n",
    "    language='en',  # we set the language to english\n",
    "    user_agent='BreakingBadNetwork/1.0'  # we insert a user-agent string for wiki\n",
    ")\n",
    "\n",
    "# we fetch a specific article\n",
    "page = wiki_wiki.page('List of characters in the Breaking Bad franchise')\n",
    "\n",
    "# we check if the article exists and print the entire content\n",
    "if page.exists():\n",
    "    article_content = page.text # saves the article as variable\n",
    "    print(\"Title: \", page.title)\n",
    "    print(\"Content: \", page.text[:100])  # we retrieve the entire text of the article\n",
    "else:\n",
    "    print(\"the article does not exist\")\n",
    "\n",
    "# save as a file\n",
    "with open(\"wiki_breaking_bad_characters.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(page.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context / Prompt creation ðŸ›£ï¸\n",
    "\n",
    " * We will need some context for the LLM when the manuscript is going to be processed. \n",
    " * For that - we will use the Wikipedia article \"List of characters in the Breaking Bad franchise\" \n",
    "     * (https://en.wikipedia.org/wiki/List_of_characters_in_the_Breaking_Bad_franchise)\n",
    " * The purpose of it is to ensure that the LLM understands which characters is present during the processing of the manuscript\n",
    " * The result: Improvement of the network creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create LLM summary of characters to be used as part of the prompt during processing of the manuscript (subtitles) ðŸŽ›ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "hBoLnYgDJ8VY"
   },
   "outputs": [],
   "source": [
    "SUM_PROMPT = \"\"\"\n",
    "You are an expert analyst of fictional characters. Your task is to summarize key information about characters from the Breaking Bad universe. For each character provided, you should:\n",
    "\n",
    "1. State their name\n",
    "2. Describe their primary role in the story\n",
    "3. Outline their key relationships to other characters\n",
    "\n",
    "Your summary should be concise yet informative, focusing only on the most important aspects of each character. Avoid including any information not explicitly provided in the input. If you're unsure about any details, do not speculate.\n",
    "\n",
    "Format your response as a bullet-point list, with each character as a main point and their details as sub-points. You need to focus on that you state the different variances of the characters names in the summary.\n",
    "\n",
    "Example format:\n",
    "â€¢ Character Name:\n",
    "  - Role: [Brief description of their role]\n",
    "  - Key Relationships: [List of important relationships]\n",
    "\n",
    "Provide this summary based solely on the information given in the input, without adding any external knowledge about Star Wars.\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model='meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo', \n",
    "    messages=[\n",
    "        {'role': 'system', 'content': SUM_PROMPT},\n",
    "        {'role': 'user', 'content': f\"Summarize these Breaking Bad characters:\\n\\n{article_content}\"}\n",
    "    ],\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "article_sum = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"â€¢ Walter White (also known by his alias Heisenberg):\\n  - Role: A high school chemistry teacher turned methamphetamine manufacturer and dealer.\\n  - Key Relationships: Skyler White (wife), Walter Jr. (son), Jesse Pinkman (business partner), Hank Schrader (brother-in-law), Saul Goodman (lawyer), Mike Ehrmantraut (associate).\\n\\nâ€¢ Skyler White:\\n  - Role: Walter's wife, who becomes involved in his money laundering activities.\\n  - Key Relationships: Walter White (husband), Walter Jr. (son), Hank Schrader (brother-in-law), Marie Schrader (sister), Saul Goodman (lawyer).\\n\\nâ€¢ Jesse Pinkman:\\n  - Role: A small-time methamphetamine user, manufacturer, and dealer who becomes Walter's business partner.\\n  - Key Relationships: Walter White (business partner), Andrea Cantillo (girlfriend), Brock Cantillo (Andrea's son), Saul Goodman (lawyer), Mike Ehrmantraut (associate).\\n\\nâ€¢ Hank Schrader:\\n  - Role: A U.S. Drug Enforcement Administration (DEA) agent and Walter's brother-in-law.\\n  - Key Relationships: Walt\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_sum[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manuscript (subtitles) ðŸ“ --> LLM Processing âš™ï¸ --> JSON schema of characters, events, locations & seasons ðŸ“¦\n",
    "\n",
    "* Here we use the create the system prompt for the LLM - It contains infomation of what we want the LLM to look for\n",
    "* Furthermore, we specify \"Background Information\" to be the earlier LLM-processed summary of the Wikipedia article as context for the TV-show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Il4iwOWhDMwB"
   },
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = f\"\"\"\n",
    "You are an assistant specialized in analyzing and structuring information about TV series. Your task is to help build a network of relationships between various entities in a given TV series, based on the following summary:\n",
    "\n",
    "Background Information:\n",
    "{article_sum}\n",
    "\n",
    "This series features a range of characters involved in complex relationships. Your primary goal is to analyze these connections and structure them into JSON format.\n",
    "\n",
    "Your task includes:\n",
    "1. Identifying relevant entities such as characters, locations, events, and seasons in the series.\n",
    "2. Establishing meaningful relationships between these entities, noting when each relationship occurs (season).\n",
    "\n",
    "Key Guidelines:\n",
    "- Each entity should have a unique name and a defined type (e.g., 'character', 'location').\n",
    "- Relationships must always specify the source entity, target entity, relationship type, and season.\n",
    "- Use only predefined relationship types provided.\n",
    "\n",
    "Additionally, you should:\n",
    "- Be able to answer questions about the structure and relationships in the series.\n",
    "- Offer suggestions for expanding or refining the network.\n",
    "- Identify central characters, significant events, and key locations, using network connections as a basis for insight into the series' narrative structure and character development.\n",
    "\n",
    "Explain your choices and reasoning as needed, ensuring that your analysis aids in understanding the seriesâ€™ narrative structure over time.\n",
    "\n",
    "Output JSON only.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mRTX4gx7TncZ",
    "outputId": "24db4153-24da-431d-b558-1bc668a7b9a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing episode Season_5B - Buried subtitles.txt: Invalid control character at: line 36 column 33 (char 763)\n",
      "\n",
      "Analysis complete! Results saved to 'breaking_bad_analysis.json'\n",
      "Processed 61 episodes\n",
      "\n",
      "Sample of the data:\n",
      "{\n",
      "    \"Season_1 - Season_1 - ...and the Bag's in the River subtitles.txt\": {\n",
      "        \"entities\": [\n",
      "            {\n",
      "                \"name\": \"Walter White\",\n",
      "                \"type\": \"Character\"\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"Classroom\",\n",
      "                \"type\": \"Location\"\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"Chemistry Lesson\",\n",
      "                \"type\": \"Event\"\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"Season 1\",\n",
      "                \"type\": \"Season\"\n",
      "            }\n",
      "        ],\n",
      "        \"relationships\": [\n",
      "            {\n",
      "                \"source\": \"Walter White\",\n",
      "                \"relation\": \"teaches\",\n",
      "                \"target\": \"Chemistry Lesson\",\n",
      "                \"season\": 1\n",
      "            },\n",
      "            {\n",
      "                \"source\": \"Walter White\",\n",
      "                \"relation\": \"works in\",\n",
      "                \"target\": \"Classroom\",\n",
      "                \"season\": 1\n",
      "            },\n",
      "            {\n",
      "                \"source\": \"Chemistry Lesson\",\n",
      "                \"relation\": \"occurs in\",\n",
      "                \"target\": \"Classroom\",\n",
      "                \"season\": 1\n",
      "            },\n",
      "            {\n",
      "                \"source\": \"Chemistry Lesson\",\n",
      "                \"relation\": \"is part of\",\n",
      "                \"target\": \"Season 1\",\n",
      "                \"season\": 1\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Dict, Any\n",
    "\n",
    "# path to the subtitles directory\n",
    "subtitles_dir = 'subtitles'\n",
    "\n",
    "def extract_relationships(script_content: str, episode_name: str, season_number: int) -> Dict[str, Any]:\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the following Breaking Bad episode script and identify entities and their relationships.\n",
    "    Episode: {episode_name}\n",
    "    Season: {season_number}\n",
    "\n",
    "    Please output ONLY a valid JSON object following exactly this schema:\n",
    "    {{\n",
    "        \"entities\": [\n",
    "            {{\n",
    "                \"name\": \"string\",\n",
    "                \"type\": \"Character\" | \"Location\" | \"Event\" | \"Season\"\n",
    "            }}\n",
    "        ],\n",
    "        \"relationships\": [\n",
    "            {{\n",
    "                \"source\": \"string\",\n",
    "                \"relation\": \"friend of\" | \"enemy of\" | \"related to\" | \"married to\" | \"works with\" | \"lives in\" | \"visits\" | \"owns\" | \"participates in\" | \"witnesses\" | \"causes\" | \"appears in\" | \"is central to\" | \"introduces\" | \"concludes\" | \"develops\" | \"part of\",\n",
    "                \"target\": \"string\",\n",
    "                \"season\": {season_number}\n",
    "            }}\n",
    "        ]\n",
    "    }}\n",
    "\n",
    "    Script content:\n",
    "    {script_content[:1000]}...\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model='Qwen/Qwen2.5-72B-Instruct-Turbo',\n",
    "            messages=[\n",
    "                {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "                {'role': 'user', 'content': prompt}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        response_text = response.choices[0].message.content.strip()\n",
    "\n",
    "        # get response and fix weird characters\n",
    "        response_text = ''.join(char for char in response.choices[0].message.content if ord(char) >= 32 or char in '\\n\\r\\t')\n",
    "\n",
    "\n",
    "        # Ensure we get valid JSON\n",
    "        try:\n",
    "            data = json.loads(response_text)\n",
    "            return data\n",
    "        except json.JSONDecodeError:\n",
    "            # Fallback: Try to extract JSON from response\n",
    "            json_start = response_text.find('{')\n",
    "            json_end = response_text.rfind('}') + 1\n",
    "            if json_start >= 0 and json_end > json_start:\n",
    "                json_str = response_text[json_start:json_end]\n",
    "                return json.loads(json_str)\n",
    "            raise\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing episode {episode_name}: {str(e)}\")\n",
    "        return {\"entities\": [], \"relationships\": []}\n",
    "\n",
    "def analyze_all_episodes(subtitles_dir: str) -> Dict[str, Any]:\n",
    "    all_episode_data = {}\n",
    "\n",
    "    for season_dir in sorted(os.listdir(subtitles_dir)):\n",
    "        season_path = os.path.join(subtitles_dir, season_dir)\n",
    "        if os.path.isdir(season_path):\n",
    "            season_number = int(season_dir.split('_')[1]) if season_dir.split('_')[1].isdigit() else 0\n",
    "\n",
    "            for episode_file in sorted(os.listdir(season_path)):\n",
    "                if episode_file.endswith('.txt'):\n",
    "                    episode_path = os.path.join(season_path, episode_file)\n",
    "                    clean_episode_name = episode_file.replace('%27', \"'\").replace('%20', \" \")\n",
    "\n",
    "                    try:\n",
    "                        with open(episode_path, 'r', encoding='utf-8') as file:\n",
    "                            script_content = file.read()\n",
    "\n",
    "                        episode_data = extract_relationships(script_content, clean_episode_name, season_number)\n",
    "                        if episode_data[\"entities\"] or episode_data[\"relationships\"]:\n",
    "                            all_episode_data[f\"{season_dir} - {clean_episode_name}\"] = episode_data\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading file {episode_path}: {str(e)}\")\n",
    "                        continue\n",
    "\n",
    "    return all_episode_data\n",
    "\n",
    "# Analyze episodes and save results as JSON\n",
    "all_episode_data = analyze_all_episodes(subtitles_dir)\n",
    "\n",
    "# Save to JSON file\n",
    "with open('breaking_bad_analysisV2.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_episode_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Optional: Print summary of results\n",
    "print(\"\\nAnalysis complete! Results saved to 'breaking_bad_analysisV2.json'\")\n",
    "print(f\"Processed {len(all_episode_data)} episodes\")\n",
    "\n",
    "# Optional: Print sample of the data in JSON format\n",
    "print(\"\\nSample of the data:\")\n",
    "print(json.dumps(dict(list(all_episode_data.items())[:1]), indent=4))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
