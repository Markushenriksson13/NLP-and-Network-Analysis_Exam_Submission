{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breaking Bad Data Processing with LLMs üñäÔ∏è ‚öôÔ∏è üì®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook leverages Large Language Models (LLMs) - to fetch the dialogue (subtitles) of the TV series *Breaking Bad* from the Movie and TV-show wiki: *Fandom.com* and extract relationships between characters, locations, events & season number.\n",
    "\n",
    "#### 1. Data Acquisition\n",
    "\n",
    "- **Scraping**: Subtitles are scraped from the Breaking Bad Fandom Wiki for all seasons and saved as individual text files.\n",
    "- **Cleaning**: The scraped subtitles are cleaned to remove timestamps and other unnecessary elements, leaving only the dialogue.\n",
    "\n",
    "#### 2. Context and Prompt Creation\n",
    "\n",
    "- **Wikipedia Summary**: The notebook utilizes the Wikipedia article \"List of characters in the Breaking Bad franchise\" to create a summarized context of key characters and their relationships using the LLM: `meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo`. This summary is used as background knowledge for the LLM processing of the subtitles (dialogue in the TV-show).\n",
    "- **LLM Prompts**: System prompts are  defined for the LLM. These prompts guide its analysis, ensuring that the extracted information follows a predefined JSON schema for representing relationships.\n",
    "\n",
    "#### 3. LLM Processing, Extraction & Output\n",
    "\n",
    "- **Episode Analysis**: The LLM (`Qwen/Qwen2.5-72B-Instruct-Turbo`) iterates through each episode's subtitle file. The content of the subtitles, along with details like episode name and season number, are fed to the LLM as prompts.\n",
    "- **Entity and Relationship Extraction**: The LLM analyzes the script and extracts entities (characters, locations, events). It then identifies relationships between these entities using a set of predefined relationship types (e.g., \"friend of,\" \"enemy of,\" \"works with\").\n",
    "- **JSON Structuring**: The extracted information is structured into a JSON format, for easy storage and further analysis...\n",
    "\n",
    "- **JSON Output**: All the LLM-processed episode data is saved into a single JSON file named \"breaking_bad_analysisV2.json\".\n",
    "- **Summary**: A summary indicating the number of processed episodes is displayed to the user along with failed episodes (if it unlikely would occur)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_hvW0WOLCxl"
   },
   "source": [
    "### Install & Import Libraries üéõÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UX4ZBS_FV99u",
    "outputId": "7664b6a4-c2a9-4ec5-aeb9-19400add0bcb"
   },
   "outputs": [],
   "source": [
    "# Install required packages from requirements.txt\n",
    "!pip install -r https://raw.githubusercontent.com/Markushenriksson13/NLP-and-Network-Analysis_Exam_Submission/refs/heads/main/requirements.txt -q\n",
    "\n",
    "# importing the clear_output function from IPython.display module to reduce noise in outputs...\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Datascrapping libs\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Datahandling\n",
    "import os\n",
    "import re\n",
    "\n",
    "# LLM Libs & Setup\n",
    "from openai import OpenAI\n",
    "import json\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "\n",
    "# Wikipedia import\n",
    "import wikipediaapi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API SETUP (User Together API-Key Input required)... üìù üìù üìù\n",
    "For the LLM-processing you will need to input a Together API Key in the field \"INSERT TOKEN\" below\n",
    " * (https://api.together.ai/signin)\n",
    "\n",
    " Please note that you can use Google Colab SECRETS, if you have saved your Together API Key there - \n",
    " * If yes? (Colab): \n",
    "    * 1. Remove the #'s below for Colab lib loading & TOGETHER_API_KEY = userdata.get('TOGETHER_API_KEY')\n",
    "    * 2. Remove the other line: TOGETHER_API_KEY = \"INSERT TOKEN\" or use #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup OpenAI client with custom TogetherAPI key and base URL\n",
    "\n",
    "# FOR COLAB YOU CAN USE GOOGLE COLAB SECRETS, if you have saved your Together API Key there - \n",
    "#  - If yes?: Remove the #'s below and remove the other line: TOGETHER_API_KEY = \"INSERT TOKEN\"\n",
    "\n",
    "# from google.colab import userdata\n",
    "# TOGETHER_API_KEY = userdata.get('TOGETHER_API_KEY')\n",
    "\n",
    "TOGETHER_API_KEY = \"INSERT TOKEN\" # INSERT YOUR TOKEN\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.together.xyz/v1\",\n",
    "    api_key=TOGETHER_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KbIw0o9ALCxn"
   },
   "source": [
    "### Setup + Data Extraction ‚öôÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "337iB-HZLCxo"
   },
   "source": [
    "#### Fetching Breaking Bad Data from Fandom (using subtitles of each season/episode) üìñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "17hWbYkCLCxp",
    "outputId": "4aba9819-3b8d-46df-c3d4-097b6ec14803"
   },
   "outputs": [],
   "source": [
    "# url for breaking bad subs\n",
    "base_url = \"https://breakingbad.fandom.com/wiki/Category:Breaking_Bad_Subtitles\"\n",
    "\n",
    "# we define functions to extract data from each season/episode from fandom.com \n",
    "# subtitles are located inside a <pre></pre> tag on each page - which we tells BeautifulSoup to parse (inside html) in functions get_season_links & get_episode_links\n",
    "# when episode_url has been specified - we tell soup to find the subtitles inside a <pre></pre> tag\n",
    "def get_season_links(base_url, target_seasons):\n",
    "    response = requests.get(base_url)  # get the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')  # parse it\n",
    "    season_links = []  # list for links\n",
    "\n",
    "    for link in soup.select('a.category-page__member-link'):  # find links\n",
    "        for season in target_seasons:  # check seasons\n",
    "            if f\"Season_{season}\" in link['href']:  # if it's a season\n",
    "                season_links.append(\"https://breakingbad.fandom.com\" + link['href'])  # add link\n",
    "    return season_links  # return all links\n",
    "\n",
    "def get_episode_links(season_url):\n",
    "    response = requests.get(season_url)  # get season page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')  # parse it - for episode links\n",
    "    episode_links = []  # list for episodes\n",
    "    for link in soup.select('a.category-page__member-link'):  # find episode links\n",
    "        episode_links.append(\"https://breakingbad.fandom.com\" + link['href'])  # add to list\n",
    "    return episode_links  # return episode links\n",
    "\n",
    "def get_subtitles(episode_url):\n",
    "    response = requests.get(episode_url)  # get episode page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')  # parse it - for season links\n",
    "    subtitle_pre = soup.find(\"pre\")  # find subtitles\n",
    "    if subtitle_pre:  # if found\n",
    "        subtitles = subtitle_pre.get_text(strip=True)  # get text\n",
    "        return subtitles  # return subtitles\n",
    "    return \"\"  # return empty if not found\n",
    "\n",
    "def save_subtitles(episode_name, subtitles, season):\n",
    "    # handle season 5A and 5B\n",
    "    season_folder = f\"Season_{season}\"  # folder name\n",
    "    os.makedirs(f\"subtitles/{season_folder}\", exist_ok=True)  # make folder\n",
    "    file_path = f\"subtitles/{season_folder}/{season_folder} - {episode_name}.txt\"  # file path\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:  # open file\n",
    "        file.write(subtitles)  # write subtitles\n",
    "\n",
    "def scrape_and_save_subtitles():\n",
    "    target_seasons = [1, 2, 3, 4, \"5A\", \"5B\"]  # all seasons\n",
    "    # if only season 1 wanted: target_seasons = [1] \n",
    "    season_links = get_season_links(base_url, target_seasons)  # get season links\n",
    "\n",
    "    for season_url in season_links:  # for each season\n",
    "        season = None  # reset season\n",
    "        for s in target_seasons:  # check seasons\n",
    "            if f\"Season_{s}\" in season_url:  # if found\n",
    "                season = s  # set season\n",
    "                break  # exit loop\n",
    "\n",
    "        if season:  # if season is set\n",
    "            episode_links = get_episode_links(season_url)  # get episodes\n",
    "            for episode_url in episode_links:  # for each episode\n",
    "                subtitles = get_subtitles(episode_url)  # get subs\n",
    "                episode_name = episode_url.split(\"/\")[-1].replace(\"_\", \" \")  # get name\n",
    "                save_subtitles(episode_name, subtitles, season)  # save subs\n",
    "                print(f\"Saved subs for {episode_name} in Season {season}\")  # print status\n",
    "\n",
    "# run the scraper\n",
    "scrape_and_save_subtitles()\n",
    "\n",
    "clear_output()  # clear output to remove obsolete noise from output\n",
    "print(\"Manuscript Saved!\")  # done!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CUUwsHp9Mbfk"
   },
   "source": [
    "#### Data Cleaning üßπ üßπ üßπ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * Raw subtitles data needs to be cleaned before processing, since they contain a lot of noise in terms of timestamps and numbers. \n",
    "\n",
    "Example:\n",
    "\n",
    "```bash\n",
    "1\n",
    "00:00:03,762 --> 00:00:05,264\n",
    "In closing, I can tell you...\n",
    "```\n",
    "\n",
    " * We need to remove unnecessary elements and irrelevant information to focus solely on the spoken content. This will help reduce noise for the LLM processing of the subtitles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ktdrBJWMbMr"
   },
   "outputs": [],
   "source": [
    "def clean_subtitle(input_text):\n",
    "    # remove lines with timestamps and numbers\n",
    "    lines = input_text.split('\\n')  # split text into lines\n",
    "    cleaned_lines = []  # list for cleaned lines\n",
    "    skip_next = False  # flag to skip next line\n",
    "\n",
    "    for line in lines:  # go through each line\n",
    "        if skip_next:  # if we need to skip\n",
    "            skip_next = False  # reset flag\n",
    "            continue  # move to next line\n",
    "        if re.match(r'^\\d+$', line.strip()) or '-->' in line:  # if it's a number or timestamp\n",
    "            skip_next = True  # set flag to skip next line\n",
    "            continue  # skip this line\n",
    "        if line.strip():  # if line is not empty\n",
    "            cleaned_lines.append(line.strip())  # add to cleaned lines\n",
    "\n",
    "    return ' '.join(cleaned_lines)  # join cleaned lines into one string\n",
    "\n",
    "def process_directory(root_dir):\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):  # walk through the folder\n",
    "        for filename in filenames:  # for each file\n",
    "            if filename.endswith('.txt'):  # check if it's a txt file\n",
    "                file_path = os.path.join(dirpath, filename)  # get full path\n",
    "\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:  # open file\n",
    "                    content = file.read()  # read the content\n",
    "\n",
    "                cleaned_content = clean_subtitle(content)  # clean the content\n",
    "\n",
    "                with open(file_path, 'w', encoding='utf-8') as file:  # open file to write\n",
    "                    file.write(cleaned_content)  # write cleaned content\n",
    "\n",
    "# path to the folder with subtitles\n",
    "root_directory = 'subtitles/'  # folder path\n",
    "\n",
    "process_directory(root_directory)  # start processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8xRLQ1fqLCxp"
   },
   "source": [
    "### Wikipedia Context for LLM character background infomation üåé\n",
    "\n",
    "We use the Wikipedia article **\"List of characters in the Breaking Bad franchise\"** to provide the LLM with background knowledge about the key characters and their relationships. This helps us improve the network creation by enabling the LLM to understand which characters are present and how they are connected.\n",
    "\n",
    "**Fetching the Wikipedia Article:**\n",
    "   - We import the `wikipediaapi` library\n",
    "   - We fetch the article content using `wiki_wiki.page('List of characters in the Breaking Bad franchise').text`\n",
    "   - We save the content to a file named `wiki_breaking_bad_characters.txt` for later use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JQlgke4TEnRL",
    "outputId": "2fcef5f5-8e85-4efd-83f8-8f0c5748a2b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:  List of characters in the Breaking Bad franchise\n",
      "Content:  Breaking Bad is a crime drama franchise created by American filmmaker Vince Gilligan. It started wit\n"
     ]
    }
   ],
   "source": [
    "# we initialize the wikipedia api\n",
    "wiki_wiki = wikipediaapi.Wikipedia(\n",
    "    language='en',  # we set the language to english\n",
    "    user_agent='BreakingBadNetwork/1.0'  # we insert a user-agent string for wiki\n",
    ")\n",
    "\n",
    "# we fetch a specific article\n",
    "page = wiki_wiki.page('List of characters in the Breaking Bad franchise')\n",
    "\n",
    "# we check if the article exists and print the entire content\n",
    "if page.exists():\n",
    "    article_content = page.text # saves the article as variable\n",
    "    print(\"Title: \", page.title)\n",
    "    print(\"Content: \", page.text[:100])  # we retrieve the entire text of the article\n",
    "else:\n",
    "    print(\"the article does not exist\")\n",
    "\n",
    "# save as a file\n",
    "with open(\"wiki_breaking_bad_characters.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(page.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context / Prompt creation üõ£Ô∏è\n",
    "\n",
    "* We will need some context for the LLM when the manuscript is processed.\n",
    "* To achieve this, we will use the Wikipedia article **\"List of characters in the Breaking Bad franchise**\" \n",
    "\n",
    "  * [Wiki: List of characters in the Breaking Bad franchise](https://en.wikipedia.org/wiki/List_of_characters_in_the_Breaking_Bad_franchise)\n",
    "\n",
    "* We create a prompt (**SUM_PROMPT** below) to instruct the LLM to summarize the characters and their relationships.\n",
    "* The LLM (**`meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo`**) is called to process the article content and create a summary. We use a different LLM for summary of the wiki-article because it has a higher Token input level than the Qwen-model used later for processing of subtitles.\n",
    "* The generated summary of the wiki-article is stored in the **article_sum** variable.\n",
    "  \n",
    "The purpose of this is to ensure that the LLM understands which characters are present during the processing of the manuscript. The result will be an improvement in the network creation when we're going to process each subtitle.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create LLM summary of characters to be used as part of the prompt during processing of the manuscript (subtitles) üéõÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hBoLnYgDJ8VY"
   },
   "outputs": [],
   "source": [
    "SUM_PROMPT = \"\"\"\n",
    "You are an expert analyst of fictional characters. Your task is to summarize key information about characters from the Breaking Bad universe. For each character provided, you should:\n",
    "\n",
    "1. State their name\n",
    "2. Describe their primary role in the story\n",
    "3. Outline their key relationships to other characters\n",
    "\n",
    "Your summary should be concise yet informative, focusing only on the most important aspects of each character. Avoid including any information not explicitly provided in the input. If you're unsure about any details, do not speculate.\n",
    "\n",
    "Format your response as a bullet-point list, with each character as a main point and their details as sub-points. You need to focus on that you state the different variances of the characters names in the summary.\n",
    "\n",
    "Example format:\n",
    "‚Ä¢ Character Name:\n",
    "  - Role: [Brief description of their role]\n",
    "  - Key Relationships: [List of important relationships]\n",
    "\n",
    "Provide this summary based solely on the information given in the input, without adding any external knowledge about Star Wars.\n",
    "\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model='meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo', \n",
    "    messages=[\n",
    "        {'role': 'system', 'content': SUM_PROMPT},\n",
    "        {'role': 'user', 'content': f\"Summarize these Breaking Bad characters:\\n\\n{article_content}\"}\n",
    "    ],\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "article_sum = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"‚Ä¢ Walter White (also known by his alias Heisenberg):\\n  - Role: A high school chemistry teacher turned methamphetamine manufacturer and dealer.\\n  - Key Relationships: Skyler White (wife), Walter Jr. (son), Jesse Pinkman (business partner), Hank Schrader (brother-in-law), Saul Goodman (lawyer), Mike Ehrmantraut (associate).\\n\\n‚Ä¢ Skyler White:\\n  - Role: Walter's wife, who becomes involved in his money laundering activities.\\n  - Key Relationships: Walter White (husband), Walter Jr. (son), Hank Schrader (brother-in-law), Marie Schrader (sister), Saul Goodman (lawyer).\\n\\n‚Ä¢ Jesse Pinkman:\\n  - Role: A small-time methamphetamine user, manufacturer, and dealer who becomes Walter's business partner.\\n  - Key Relationships: Walter White (business partner), Andrea Cantillo (girlfriend), Brock Cantillo (Andrea's son), Saul Goodman (lawyer), Mike Ehrmantraut (associate).\\n\\n‚Ä¢ Hank Schrader:\\n  - Role: A U.S. Drug Enforcement Administration (DEA) agent and Walter's brother-in-law.\\n  - Key Relationships: Walt\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_sum[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manuscript (subtitles) üìù --> LLM Processing ‚öôÔ∏è --> JSON schema of characters, events, locations & seasons üì¶\n",
    "This section is where we dive into the core analysis of the Breaking Bad subtitles, transforming unstructured text into structured data\n",
    "\n",
    "##### Manuscript (Subtitles) Preparation üìù\n",
    "\n",
    "* We start by loading the cleaned subtitle files from the **'subtitles'** directory, one by one\n",
    "* Each file represents the dialogue from a specific episode of Breaking Bad\n",
    "\n",
    "##### LLM Processing ‚öôÔ∏è\n",
    "\n",
    "* **System Prompt**: We provide a detailed prompt (**SYSTEM_PROMPT**) to the LLM (**`Qwen/Qwen2.5-72B-Instruct-Turbo`**). This prompt includes instructions to:\n",
    "  * Analyze the episode script.\n",
    "  * Identify characters, locations, events, and the season.\n",
    "  * Establish relationships between these entities using predefined relationship types.\n",
    "  * Structure the extracted information into a JSON format following a specific schema.\n",
    "\n",
    "* **Background Context**: The **SYSTEM_PROMPT** also includes the character summary we derived from Wikipedia (**article_sum**) to give the LLM some context.\n",
    "\n",
    "* **Episode Analysis**: For each episode, we feed the script content, episode name, and season number to the LLM.\n",
    "\n",
    "* **Extraction**: The LLM processes the script and extracts entities and their relationships based on the instructions in the prompt and the context we provided. We also account for errors that may occur during the LLM-processing.\n",
    "\n",
    "##### JSON Schema Output üì¶\n",
    "\n",
    "* **Structuring**: We organize the extracted information into a JSON format according to the predefined schema.\n",
    "  * **entities**: This contains a list of identified characters, locations, events, and seasons, each with a name and type.\n",
    "  * **relationships**: This includes a list of relationships between entities, detailing the source, relation, target, and season.\n",
    "\n",
    "* **Saving**: Finally, we save all the LLM-processed episode data into a single JSON file named **\"breaking_bad_analysisV2.json\"**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### System Prompt ü§ñ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Il4iwOWhDMwB"
   },
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = f\"\"\"\n",
    "You are an assistant specialized in analyzing and structuring information about TV series. Your task is to help build a network of relationships between various entities in a given TV series, based on the following summary:\n",
    "\n",
    "Background Information:\n",
    "{article_sum}\n",
    "\n",
    "This series features a range of characters involved in complex relationships. Your primary goal is to analyze these connections and structure them into JSON format.\n",
    "\n",
    "Your task includes:\n",
    "1. Identifying relevant entities such as characters, locations, events, and seasons in the series.\n",
    "2. Establishing meaningful relationships between these entities, noting when each relationship occurs (season).\n",
    "\n",
    "Key Guidelines:\n",
    "- Each entity should have a unique name and a defined type (e.g., 'character', 'location').\n",
    "- Relationships must always specify the source entity, target entity, relationship type, and season.\n",
    "- Use only predefined relationship types provided.\n",
    "\n",
    "Additionally, you should:\n",
    "- Be able to answer questions about the structure and relationships in the series.\n",
    "- Offer suggestions for expanding or refining the network.\n",
    "- Identify central characters, significant events, and key locations, using network connections as a basis for insight into the series' narrative structure and character development.\n",
    "\n",
    "Explain your choices and reasoning as needed, ensuring that your analysis aids in understanding the series‚Äô narrative structure over time.\n",
    "\n",
    "Output JSON only.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Definition of JSON Schema & LLM-processing of subtitles üéõÔ∏è ‚öôÔ∏è üì¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mRTX4gx7TncZ",
    "outputId": "24db4153-24da-431d-b558-1bc668a7b9a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing episode Season_5B - Buried subtitles.txt: Invalid control character at: line 36 column 33 (char 763)\n",
      "\n",
      "Analysis complete! Results saved to 'breaking_bad_analysis.json'\n",
      "Processed 61 episodes\n",
      "\n",
      "Sample of the data:\n",
      "{\n",
      "    \"Season_1 - Season_1 - ...and the Bag's in the River subtitles.txt\": {\n",
      "        \"entities\": [\n",
      "            {\n",
      "                \"name\": \"Walter White\",\n",
      "                \"type\": \"Character\"\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"Classroom\",\n",
      "                \"type\": \"Location\"\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"Chemistry Lesson\",\n",
      "                \"type\": \"Event\"\n",
      "            },\n",
      "            {\n",
      "                \"name\": \"Season 1\",\n",
      "                \"type\": \"Season\"\n",
      "            }\n",
      "        ],\n",
      "        \"relationships\": [\n",
      "            {\n",
      "                \"source\": \"Walter White\",\n",
      "                \"relation\": \"teaches\",\n",
      "                \"target\": \"Chemistry Lesson\",\n",
      "                \"season\": 1\n",
      "            },\n",
      "            {\n",
      "                \"source\": \"Walter White\",\n",
      "                \"relation\": \"works in\",\n",
      "                \"target\": \"Classroom\",\n",
      "                \"season\": 1\n",
      "            },\n",
      "            {\n",
      "                \"source\": \"Chemistry Lesson\",\n",
      "                \"relation\": \"occurs in\",\n",
      "                \"target\": \"Classroom\",\n",
      "                \"season\": 1\n",
      "            },\n",
      "            {\n",
      "                \"source\": \"Chemistry Lesson\",\n",
      "                \"relation\": \"is part of\",\n",
      "                \"target\": \"Season 1\",\n",
      "                \"season\": 1\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Dict, Any\n",
    "\n",
    "# path to the subtitles folder\n",
    "subtitles_dir = 'subtitles'  # where the subs are\n",
    "\n",
    "def extract_relationships(script_content: str, episode_name: str, season_number: int) -> Dict[str, Any]:\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the Breaking Bad script and find entities and their relationships.\n",
    "    Episode: {episode_name}\n",
    "    Season: {season_number}\n",
    "\n",
    "    Output ONLY a valid JSON object like this:\n",
    "    {{\n",
    "        \"entities\": [\n",
    "            {{\n",
    "                \"name\": \"string\",\n",
    "                \"type\": \"Character\" | \"Location\" | \"Event\" | \"Season\"\n",
    "            }}\n",
    "        ],\n",
    "        \"relationships\": [\n",
    "            {{\n",
    "                \"source\": \"string\",\n",
    "                \"relation\": \"friend of\" | \"enemy of\" | \"related to\" | \"married to\" | \"works with\" | \"lives in\" | \"visits\" | \"owns\" | \"participates in\" | \"witnesses\" | \"causes\" | \"appears in\" | \"is central to\" | \"introduces\" | \"concludes\" | \"develops\" | \"part of\",\n",
    "                \"target\": \"string\",\n",
    "                \"season\": {season_number}\n",
    "            }}\n",
    "        ]\n",
    "    }}\n",
    "\n",
    "    Script content:\n",
    "    {script_content[:1000]}...\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model='Qwen/Qwen2.5-72B-Instruct-Turbo',\n",
    "            messages=[\n",
    "                {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "                {'role': 'user', 'content': prompt}\n",
    "            ],\n",
    "            temperature=0.7\n",
    "        )\n",
    "\n",
    "        response_text = response.choices[0].message.content.strip()\n",
    "\n",
    "        # clean up weird characters\n",
    "        response_text = ''.join(char for char in response_text if ord(char) >= 32 or char in '\\n\\r\\t')\n",
    "\n",
    "        # make sure we get valid JSON\n",
    "        try:\n",
    "            data = json.loads(response_text)\n",
    "            return data\n",
    "        except json.JSONDecodeError:\n",
    "            # try to fix JSON if it fails\n",
    "            json_start = response_text.find('{')\n",
    "            json_end = response_text.rfind('}') + 1\n",
    "            if json_start >= 0 and json_end > json_start:\n",
    "                json_str = response_text[json_start:json_end]\n",
    "                return json.loads(json_str)\n",
    "            raise\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing episode {episode_name}: {str(e)}\")\n",
    "        return {\"entities\": [], \"relationships\": []}\n",
    "\n",
    "def analyze_all_episodes(subtitles_dir: str) -> Dict[str, Any]:\n",
    "    all_episode_data = {}  # store all data\n",
    "\n",
    "    for season_dir in sorted(os.listdir(subtitles_dir)):  # go through each season\n",
    "        season_path = os.path.join(subtitles_dir, season_dir)\n",
    "        if os.path.isdir(season_path):\n",
    "            season_number = int(season_dir.split('_')[1]) if season_dir.split('_')[1].isdigit() else 0\n",
    "\n",
    "            for episode_file in sorted(os.listdir(season_path)):  # check each episode\n",
    "                if episode_file.endswith('.txt'):\n",
    "                    episode_path = os.path.join(season_path, episode_file)\n",
    "                    clean_episode_name = episode_file.replace('%27', \"'\").replace('%20', \" \")\n",
    "\n",
    "                    try:\n",
    "                        with open(episode_path, 'r', encoding='utf-8') as file:\n",
    "                            script_content = file.read()  # read the script\n",
    "\n",
    "                        episode_data = extract_relationships(script_content, clean_episode_name, season_number)\n",
    "                        if episode_data[\"entities\"] or episode_data[\"relationships\"]:\n",
    "                            all_episode_data[f\"{season_dir} - {clean_episode_name}\"] = episode_data\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading file {episode_path}: {str(e)}\")\n",
    "                        continue\n",
    "\n",
    "    return all_episode_data\n",
    "\n",
    "# finally to analyse episodes and save results as JSON\n",
    "all_episode_data = analyze_all_episodes(subtitles_dir)\n",
    "\n",
    "# then savin to JSON file\n",
    "with open('breaking_bad_analysisV2.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_episode_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "#  printing summary of results\n",
    "print(\"\\nAnalysis complete! Results saved to 'breaking_bad_analysisV2.json'\")\n",
    "print(f\"Processed {len(all_episode_data)} episodes\")\n",
    "\n",
    "# also printing sample of the data in the JSON format we want to work with\n",
    "print(\"\\nSample of the data:\")\n",
    "print(json.dumps(dict(list(all_episode_data.items())[:1]), indent=4))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
