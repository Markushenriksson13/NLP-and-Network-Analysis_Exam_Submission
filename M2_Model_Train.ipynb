{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breaking Bad Relationship Extraction with SetFit üßë‚Äçüî¨\n",
    "\n",
    "In this notebook, we dive into extracting relationships from the *Breaking Bad* TV series using the SetFit model training.\n",
    "\n",
    "### üìÇ Data and Setup\n",
    "- We start with importing the LLM-preprocessed JSON data stored in `breaking_bad_analysisV2.json` generated from the notebook: ` M2_LLM_Data_Fetch_and_Processing_(JSON_Creation).ipynb` \n",
    "    * Online downloading from github as standard (- Offline option also available)\n",
    "- Our focus is on training and evaluating a SetFit model to classify relationships between characters.\n",
    "\n",
    "### üß† Model Selection and Fine-Tuning\n",
    "- **Base Model**: We use the `sentence-transformers/paraphrase-mpnet-base-v2` model.\n",
    "- **Fine-Tuning**: The model is fine-tuned on our dataset.\n",
    "- **Data Split**: We allocate 80% for training and 20% for evaluation.\n",
    "\n",
    "### ‚ö° Efficient Execution on Colab\n",
    "- Use **Colab GPU** (T4 GPU) to speed up the process - with an estimated runtime of ~5-6 minutes.\n",
    "\n",
    "### üíæ Model Saving and Reusability\n",
    "- The trained model is saved in the `saved_model` directory for future use.\n",
    "\n",
    "### ‚úÖ The model is utilized in a Gradio Interface in the other Notebook: \n",
    "#### ‚û°Ô∏è ‚û°Ô∏è ‚û°Ô∏è `M2_Main_Network_Analysis_and_Text_Classification.ipynb`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KnHcHYEC3yTy"
   },
   "source": [
    "### Install & Import Libraries needed for model training üéõÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jEGgseRf3yTz",
    "outputId": "eebdd546-75b6-40b1-aec0-de5f8d19db7a"
   },
   "outputs": [],
   "source": [
    "# Install required packages from requirements.txt\n",
    "!pip install -r https://raw.githubusercontent.com/Markushenriksson13/NLP-and-Network-Analysis_Exam_Submission/refs/heads/main/requirements.txt -q\n",
    "\n",
    "# import libs\n",
    "import json\n",
    "import splitfile\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from setfit import SetFitModel, SetFitTrainer\n",
    "from sklearn.metrics import classification_report\n",
    "import requests\n",
    "import os\n",
    "from filesplit.split import Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m9yxYzB53yT1"
   },
   "source": [
    "### SetFit model for training üßÆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sjruumOG3yT1",
    "outputId": "36981b29-d8ce-4322-f7ca-c576a4d4ba64"
   },
   "outputs": [],
   "source": [
    "# ONLINE DOWNLOADING JSON file from Github repository\n",
    "# URL to JSON-file\n",
    "url = 'https://raw.githubusercontent.com/Markushenriksson13/NLP-and-Network-Analysis_Exam_Submission/main/breaking_bad_analysisV2.json'\n",
    "\n",
    "# Download JSON-file\n",
    "response = requests.get(url)\n",
    "data = response.json()  # Convert to JSON-format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "tPE32qkX3yT1"
   },
   "outputs": [],
   "source": [
    "# OFFLINE LOADING OF JSON:\n",
    "#with open(\"breaking_bad_analysisV2.json\", 'r', encoding='utf-8') as file:\n",
    "#    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lfFqsd2Z3yT2"
   },
   "outputs": [],
   "source": [
    "def load_and_prepare_data():\n",
    "    \"\"\"Load JSON and prepare data for classification\"\"\"\n",
    "\n",
    "    relationships = []\n",
    "    labels = []\n",
    "\n",
    "    for episode in data.values():\n",
    "        for rel in episode.get('relationships', []):\n",
    "            relationships.append(f\"{rel['source']} - {rel['target']}\")\n",
    "            labels.append(rel['relation'])\n",
    "\n",
    "    # Split into train/test (80/20)\n",
    "    df = pd.DataFrame({'text': relationships, 'label': labels})\n",
    "    train_size = int(len(df) * 0.8)\n",
    "\n",
    "    train_data = Dataset.from_pandas(df[:train_size])\n",
    "    test_data = Dataset.from_pandas(df[train_size:])\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "def train_and_evaluate():\n",
    "    # load and prepare data\n",
    "    train_dataset, test_dataset = load_and_prepare_data()\n",
    "\n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Testing samples: {len(test_dataset)}\")\n",
    "\n",
    "    # start and train model\n",
    "    model = SetFitModel.from_pretrained(\"sentence-transformers/paraphrase-mpnet-base-v2\")\n",
    "    trainer = SetFitTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        batch_size=16,\n",
    "        num_iterations=20,\n",
    "        num_epochs=1\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # evaluate\n",
    "    predictions = model.predict(test_dataset['text'])\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(test_dataset['label'], predictions))\n",
    "\n",
    "    # Example predictions\n",
    "    print(\"\\nExample Predictions:\")\n",
    "    for text, true_label, pred_label in zip(\n",
    "        test_dataset['text'][:3],\n",
    "        test_dataset['label'][:3],\n",
    "        predictions[:3]\n",
    "    ):\n",
    "        print(f\"\\nText: {text}\")\n",
    "        print(f\"True: {true_label}\")\n",
    "        print(f\"Predicted: {pred_label}\")\n",
    "\n",
    "    # save Model\n",
    "    model.save_pretrained(\"saved_model\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# run train & evaluation\n",
    "model = train_and_evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### FOR GITHUB PUSH: Since Github has a file limit on 100 MB, we would need to split the model files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR GITHUB push\n",
    "#from filesplit.split import Split\n",
    "\n",
    "# filepath, output directory & chunk size\n",
    "#input_file = 'saved_model/model.safetensors'\n",
    "#output_dir = 'saved_model'\n",
    "#chunk_size = 100 * 1024 * 1024  # 100 MB\n",
    "\n",
    "# split model\n",
    "#splitter = Split(input_file, output_dir)\n",
    "#splitter.bysize(chunk_size)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
