{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SetFit model training using the created JSON file \n",
    "#### 'breaking_bad_analysisV2.json' from (M2_LLM_Data_Fetch_and_Processing.ipynb)\n",
    "\n",
    " * Model used: sentence-transformers/paraphrase-mpnet-base-v2\n",
    "\n",
    " * Train/Test split: 80/20\n",
    "\n",
    " * Runtime (Colab GPU T4): Aprox. 8 minuttes \n",
    "\n",
    " * Make sure to use GPU for training and testing\n",
    "\n",
    " * Saves trained model in the folder 'saved_model'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install & Import Libraries needed for model training ðŸŽ›ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/pty.py:95: DeprecationWarning: This process (pid=90396) is multi-threaded, use of forkpty() may lead to deadlocks in the child.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    }
   ],
   "source": [
    "# Install required packages from requirements.txt\n",
    "!pip install -r https://raw.githubusercontent.com/Markushenriksson13/NLP-and-Network-Analysis_Exam_Submission/refs/heads/main/requirements.txt -q\n",
    "\n",
    "# import libs\n",
    "import json\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from setfit import SetFitModel, SetFitTrainer\n",
    "from sklearn.metrics import classification_report\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SetFit model for training ðŸ§® "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONLINE DOWNLOADING JSON file from Github repository\n",
    "# URL to JSON-file\n",
    "url = 'https://raw.githubusercontent.com/Markushenriksson13/NLP-and-Network-Analysis_Exam_Submission/main/breaking_bad_analysisV2.json'\n",
    "\n",
    "# Download JSON-file\n",
    "response = requests.get(url)\n",
    "data = response.json()  # Convert to JSON-format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OFFLINE LOADING OF JSON:\n",
    "#with open(\"breaking_bad_analysisV2.json\", 'r', encoding='utf-8') as file:\n",
    "#    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 475\n",
      "Testing samples: 119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "/var/folders/z7/j9rhznl118zbt_41ygz3rgyc0000gn/T/ipykernel_90396/853941217.py:30: DeprecationWarning: `SetFitTrainer` has been deprecated and will be removed in v2.0.0 of SetFit. Please use `Trainer` instead.\n",
      "  trainer = SetFitTrainer(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/executing/executing.py:713: DeprecationWarning: ast.Str is deprecated and will be removed in Python 3.14; use ast.Constant instead\n",
      "  right=ast.Str(s=sentinel),\n",
      "/opt/anaconda3/lib/python3.12/ast.py:587: DeprecationWarning: Attribute s is deprecated and will be removed in Python 3.14; use value instead\n",
      "  return Constant(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 30] Read-only file system: 'tmp_trainer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 62\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# run train & evaluation\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m model \u001b[38;5;241m=\u001b[39m train_and_evaluate()\n",
      "Cell \u001b[0;32mIn[6], line 30\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# start and train model\u001b[39;00m\n\u001b[1;32m     29\u001b[0m model \u001b[38;5;241m=\u001b[39m SetFitModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence-transformers/paraphrase-mpnet-base-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SetFitTrainer(\n\u001b[1;32m     31\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     32\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[1;32m     33\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m     34\u001b[0m     num_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m     35\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     36\u001b[0m )\n\u001b[1;32m     38\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# evaluate\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/setfit/trainer.py:873\u001b[0m, in \u001b[0;36mSetFitTrainer.__init__\u001b[0;34m(self, model, train_dataset, eval_dataset, model_init, metric, metric_kwargs, loss_class, num_iterations, num_epochs, learning_rate, batch_size, seed, column_mapping, use_amp, warmup_proportion, distance_metric, margin, samples_per_label)\u001b[0m\n\u001b[1;32m    853\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`SetFitTrainer` has been deprecated and will be removed in v2.0.0 of SetFit. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease use `Trainer` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m    857\u001b[0m     stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    858\u001b[0m )\n\u001b[1;32m    859\u001b[0m args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m    860\u001b[0m     num_iterations\u001b[38;5;241m=\u001b[39mnum_iterations,\n\u001b[1;32m    861\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39mnum_epochs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    871\u001b[0m     loss\u001b[38;5;241m=\u001b[39mloss_class,\n\u001b[1;32m    872\u001b[0m )\n\u001b[0;32m--> 873\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    874\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    875\u001b[0m     args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m    876\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[1;32m    877\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39meval_dataset,\n\u001b[1;32m    878\u001b[0m     model_init\u001b[38;5;241m=\u001b[39mmodel_init,\n\u001b[1;32m    879\u001b[0m     metric\u001b[38;5;241m=\u001b[39mmetric,\n\u001b[1;32m    880\u001b[0m     metric_kwargs\u001b[38;5;241m=\u001b[39mmetric_kwargs,\n\u001b[1;32m    881\u001b[0m     column_mapping\u001b[38;5;241m=\u001b[39mcolumn_mapping,\n\u001b[1;32m    882\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/setfit/trainer.py:328\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, train_dataset, eval_dataset, model_init, metric, metric_kwargs, callbacks, column_mapping)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhp_search_backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    327\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m callbacks \u001b[38;5;241m+\u001b[39m [ModelCardCallback(\u001b[38;5;28mself\u001b[39m)] \u001b[38;5;28;01mif\u001b[39;00m callbacks \u001b[38;5;28;01melse\u001b[39;00m [ModelCardCallback(\u001b[38;5;28mself\u001b[39m)]\n\u001b[0;32m--> 328\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mst_trainer \u001b[38;5;241m=\u001b[39m BCSentenceTransformersTrainer(\n\u001b[1;32m    329\u001b[0m     setfit_model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    330\u001b[0m     setfit_args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs,\n\u001b[1;32m    331\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    332\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/setfit/trainer.py:48\u001b[0m, in \u001b[0;36mBCSentenceTransformersTrainer.__init__\u001b[0;34m(self, setfit_model, setfit_args, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setfit_args \u001b[38;5;241m=\u001b[39m setfit_args\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogs_prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(model\u001b[38;5;241m=\u001b[39msetfit_model\u001b[38;5;241m.\u001b[39mmodel_body, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply_training_arguments(setfit_args)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mcallbacks):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sentence_transformers/trainer.py:201\u001b[0m, in \u001b[0;36mSentenceTransformerTrainer.__init__\u001b[0;34m(self, model, args, train_dataset, eval_dataset, loss, evaluator, data_collator, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(eval_dataset, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(eval_dataset, DatasetDict):\n\u001b[1;32m    200\u001b[0m     eval_dataset \u001b[38;5;241m=\u001b[39m DatasetDict(eval_dataset)\n\u001b[0;32m--> 201\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    202\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_init \u001b[38;5;28;01melse\u001b[39;00m model,\n\u001b[1;32m    203\u001b[0m     args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m    204\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[1;32m    205\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[1;32m    206\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39meval_dataset,\n\u001b[1;32m    207\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m    208\u001b[0m     model_init\u001b[38;5;241m=\u001b[39mmodel_init,\n\u001b[1;32m    209\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[1;32m    210\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    211\u001b[0m     optimizers\u001b[38;5;241m=\u001b[39moptimizers,\n\u001b[1;32m    212\u001b[0m     preprocess_logits_for_metrics\u001b[38;5;241m=\u001b[39mpreprocess_logits_for_metrics,\n\u001b[1;32m    213\u001b[0m )\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# Every Sentence Transformer model can always return a loss, so we set this to True\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# to avoid having to specify it in the data collator or model's forward\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcan_return_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/trainer.py:611\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    609\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_hf_repo()\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[0;32m--> 611\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39moutput_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_collator) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_collator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollate_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)):\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `data_collator` should be a simple callable (function, class with `__call__`).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m<frozen os>:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system: 'tmp_trainer'"
     ]
    }
   ],
   "source": [
    "def load_and_prepare_data():\n",
    "    \"\"\"Load JSON and prepare data for classification\"\"\"\n",
    "    \n",
    "    relationships = []\n",
    "    labels = []\n",
    "\n",
    "    for episode in data.values():\n",
    "        for rel in episode.get('relationships', []):\n",
    "            relationships.append(f\"{rel['source']} - {rel['target']}\")\n",
    "            labels.append(rel['relation'])\n",
    "\n",
    "    # Split into train/test (80/20)\n",
    "    df = pd.DataFrame({'text': relationships, 'label': labels})\n",
    "    train_size = int(len(df) * 0.8)\n",
    "\n",
    "    train_data = Dataset.from_pandas(df[:train_size])\n",
    "    test_data = Dataset.from_pandas(df[train_size:])\n",
    "\n",
    "    return train_data, test_data\n",
    "\n",
    "def train_and_evaluate():\n",
    "    # load and prepare data\n",
    "    train_dataset, test_dataset = load_and_prepare_data()\n",
    "\n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Testing samples: {len(test_dataset)}\")\n",
    "\n",
    "    # start and train model\n",
    "    model = SetFitModel.from_pretrained(\"sentence-transformers/paraphrase-mpnet-base-v2\")\n",
    "    trainer = SetFitTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        batch_size=16,\n",
    "        num_iterations=20,\n",
    "        num_epochs=1\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # evaluate\n",
    "    predictions = model.predict(test_dataset['text'])\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(test_dataset['label'], predictions))\n",
    "\n",
    "    # Example predictions\n",
    "    print(\"\\nExample Predictions:\")\n",
    "    for text, true_label, pred_label in zip(\n",
    "        test_dataset['text'][:3],\n",
    "        test_dataset['label'][:3],\n",
    "        predictions[:3]\n",
    "    ):\n",
    "        print(f\"\\nText: {text}\")\n",
    "        print(f\"True: {true_label}\")\n",
    "        print(f\"Predicted: {pred_label}\")\n",
    "\n",
    "    # save Model\n",
    "    model.save_pretrained(\"saved_model\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# run train & evaluation\n",
    "model = train_and_evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
