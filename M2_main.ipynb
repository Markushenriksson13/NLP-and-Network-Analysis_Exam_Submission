{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In Class & Final Assignment\n",
    "## AI Technology Market Analysis Assignment\n",
    "### Group Project Using NLP and Network Analysis\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "Conduct a comprehensive analysis of AI technology markets by combining Natural Language Processing (NLP) and Network Analysis techniques. Use either the provided dataset or identify suitable alternative data sources that enable meaningful insights into AI market dynamics.\n",
    "\n",
    "---\n",
    "\n",
    "## Core Requirements\n",
    "\n",
    "### Data Processing with LLMs\n",
    "Implement local or cloud-based Large Language Models (LLMs) to:\n",
    "- Extract and structure relevant market data\n",
    "- Identify network relationships between entities\n",
    "- Perform named entity recognition and extraction\n",
    "- Transform unstructured text into analyzable formats\n",
    "\n",
    "### Network Analysis\n",
    "Design and construct meaningful networks from the extracted data:\n",
    "- Implement bi-partite network analysis and corresponding projections\n",
    "- Calculate and interpret key network metrics:\n",
    "  - Various centrality measures\n",
    "  - Network structure indicators\n",
    "  - Community detection (if applicable)\n",
    "- Provide clear interpretation of network analysis results\n",
    "\n",
    "### Text Classification\n",
    "Select and implement one of these approaches:\n",
    "- LLM-based classification system\n",
    "- Few-shot learning implementation using SetFit\n",
    "- Traditional NLP classification methods (using existing or synthetic training data)\n",
    "\n",
    "---\n",
    "\n",
    "## Optional Extensions\n",
    "\n",
    "### Topic Modeling\n",
    "Leverage LLMs to extract and categorize key themes and topics:\n",
    "- Apply BERTopic for advanced topic modeling\n",
    "- Create clear and insightful visualizations of:\n",
    "  - Topic distributions\n",
    "  - Theme relationships\n",
    "  - Temporal patterns (if applicable)\n",
    "\n",
    "---\n",
    "\n",
    "## Deliverables\n",
    "\n",
    "### Analysis Notebooks\n",
    "Well-documented Jupyter notebooks containing:\n",
    "- Complete analysis pipeline\n",
    "- Clear code documentation\n",
    "- Inline result interpretation\n",
    "- Reproducible implementation\n",
    "\n",
    "### Executive Summary\n",
    "Concise PDF slide deck (max 6 slides) including:\n",
    "- Problem statement and approach\n",
    "- Key findings and insights\n",
    "- Visual representation of critical results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install & Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install ollama pandas networkx matplotlib tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit:1 http://archive.ubuntu.com/ubuntu focal InRelease\n",
      "Hit:2 http://security.ubuntu.com/ubuntu focal-security InRelease\n",
      "Hit:3 http://archive.ubuntu.com/ubuntu focal-updates InRelease\n",
      "Hit:4 http://archive.ubuntu.com/ubuntu focal-backports InRelease\n",
      "Hit:5 https://dl.yarnpkg.com/debian stable InRelease\n",
      "Hit:6 https://packages.microsoft.com/repos/microsoft-ubuntu-focal-prod focal InRelease\n",
      "Hit:7 https://repo.anaconda.com/pkgs/misc/debrepo/conda stable InRelease\n",
      "Hit:8 https://packagecloud.io/github/git-lfs/ubuntu focal InRelease\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "21 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "pciutils is already the newest version (1:3.6.4-1ubuntu0.20.04.1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 21 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "# updating package list to ensure pciutils is found in the list\n",
    "!sudo apt update -q\n",
    "# pciutils installation required for ollama\n",
    "!sudo apt install -y pciutils -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Installing ollama to /usr/local\n",
      ">>> Downloading Linux amd64 bundle\n",
      "######################################################################## 100.0%                                                                 5.0%       19.0%                     73.8%\n",
      ">>> Adding ollama user to video group...\n",
      ">>> Adding current user to ollama group...\n",
      ">>> Creating ollama systemd service...\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n",
      "WARNING: No NVIDIA/AMD GPU detected. Ollama will run in CPU-only mode.\n",
      ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
      ">>> Install complete. Run \"ollama\" from the command line.\n"
     ]
    }
   ],
   "source": [
    "# install Ollama\n",
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup + Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import threading\n",
    "import subprocess\n",
    "\n",
    "def start_ollama():\n",
    "    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
    "    os.environ['OLLAMA_ORIGINS'] = '*'\n",
    "    subprocess.Popen([\"ollama\", \"serve\"])\n",
    "\n",
    "ollama_thread = threading.Thread(target=start_ollama)\n",
    "ollama_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/28 17:17:03 routes.go:1158: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://0.0.0.0:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/home/codespace/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[* http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\n",
      "time=2024-10-28T17:17:03.929Z level=INFO source=images.go:754 msg=\"total blobs: 5\"\n",
      "time=2024-10-28T17:17:03.932Z level=INFO source=images.go:761 msg=\"total unused blobs removed: 0\"\n",
      "time=2024-10-28T17:17:03.932Z level=INFO source=routes.go:1205 msg=\"Listening on [::]:11434 (version 0.3.14)\"\n",
      "time=2024-10-28T17:17:03.933Z level=INFO source=common.go:135 msg=\"extracting embedded files\" dir=/tmp/ollama1187133982/runners\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GIN] 2024/10/28 - 17:17:33 | 200 |      236.47µs |       127.0.0.1 | HEAD     \"/\"\n",
      "\u001b[?25lpulling manifest ⠋ \u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=2024-10-28T17:17:33.919Z level=INFO source=common.go:49 msg=\"Dynamic LLM libraries\" runners=\"[cuda_v11 cuda_v12 rocm_v60102 cpu cpu_avx cpu_avx2]\"\n",
      "time=2024-10-28T17:17:33.920Z level=INFO source=gpu.go:221 msg=\"looking for compatible GPUs\"\n",
      "time=2024-10-28T17:17:33.943Z level=INFO source=gpu.go:384 msg=\"no compatible GPUs were discovered\"\n",
      "time=2024-10-28T17:17:33.943Z level=INFO source=types.go:123 msg=\"inference compute\" id=0 library=cpu variant=avx2 compute=\"\" driver=0.0 name=\"\" total=\"7.7 GiB\" available=\"5.8 GiB\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠦ \u001b[?25h[GIN] 2024/10/28 - 17:17:34 | 200 |  780.800103ms |       127.0.0.1 | POST     \"/api/pull\"\n",
      "\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 2bada8a74506... 100% ▕████████████████▏ 4.7 GB                         \n",
      "pulling 66b9ea09bd5b... 100% ▕████████████████▏   68 B                         \n",
      "pulling eb4402837c78... 100% ▕████████████████▏ 1.5 KB                         \n",
      "pulling 832dd9e00a68... 100% ▕████████████████▏  11 KB                         \n",
      "pulling 2f15b3218f05... 100% ▕████████████████▏  487 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n",
      "[GIN] 2024/10/28 - 17:17:34 | 200 |      24.433µs |       127.0.0.1 | HEAD     \"/\"\n",
      "\u001b[?25lpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h[GIN] 2024/10/28 - 17:17:35 | 200 |  632.005562ms |       127.0.0.1 | POST     \"/api/pull\"\n",
      "\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling 2bada8a74506... 100% ▕████████████████▏ 4.7 GB                         \n",
      "pulling 66b9ea09bd5b... 100% ▕████████████████▏   68 B                         \n",
      "pulling eb4402837c78... 100% ▕████████████████▏ 1.5 KB                         \n",
      "pulling 832dd9e00a68... 100% ▕████████████████▏  11 KB                         \n",
      "pulling 2f15b3218f05... 100% ▕████████████████▏  487 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "# make sure to download a model\n",
    "!ollama pull qwen2.5\n",
    "!ollama pull qwen2.5 # double check (hash check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After downloading the model used for Olama, we will need to restart the ollama thread:\n",
    "def start_ollama():\n",
    "    os.environ['OLLAMA_HOST'] = '0.0.0.0:11434'\n",
    "    os.environ['OLLAMA_ORIGINS'] = '*'\n",
    "    subprocess.Popen([\"ollama\", \"serve\"])\n",
    "\n",
    "ollama_thread = threading.Thread(target=start_ollama)\n",
    "ollama_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage:\n",
      "  ollama [flags]\n",
      "  ollama [command]\n",
      "\n",
      "Available Commands:\n",
      "  serve       Start ollama\n",
      "  create      Create a model from a Modelfile\n",
      "  show        Show information for a model\n",
      "  run         Run a model\n",
      "  stop        Stop a running model\n",
      "  pull        Pull a model from a registry\n",
      "  push        Push a model to a registry\n",
      "  list        List models\n",
      "  ps          List running models\n",
      "  cp          Copy a model\n",
      "  rm          Remove a model\n",
      "  help        Help about any command\n",
      "\n",
      "Flags:\n",
      "  -h, --help      help for ollama\n",
      "  -v, --version   Show version information\n",
      "\n",
      "Use \"ollama [command] --help\" for more information about a command.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: listen tcp 0.0.0.0:11434: bind: address already in use\n"
     ]
    }
   ],
   "source": [
    "!ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetching Breaking Bad Data from Fandom (using subtitles of each season/episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved subtitles for ...and the Bag%27s in the River subtitles in Season 1\n",
      "Saved subtitles for A No-Rough-Stuff-Type Deal subtitles in Season 1\n",
      "Saved subtitles for Cancer Man subtitles in Season 1\n",
      "Saved subtitles for Cat%27s in the Bag... subtitles in Season 1\n",
      "Saved subtitles for Crazy Handful of Nothin%27 subtitles in Season 1\n",
      "Saved subtitles for Gray Matter subtitles in Season 1\n",
      "Saved subtitles for Pilot subtitles in Season 1\n",
      "Saved subtitles for 4 Days Out subtitles in Season 2\n",
      "Saved subtitles for ABQ subtitles in Season 2\n",
      "Saved subtitles for Better Call Saul subtitles in Season 2\n",
      "Saved subtitles for Bit by a Dead Bee subtitles in Season 2\n",
      "Saved subtitles for Breakage subtitles in Season 2\n",
      "Saved subtitles for Down subtitles in Season 2\n",
      "Saved subtitles for Grilled subtitles in Season 2\n",
      "Saved subtitles for Mandala subtitles in Season 2\n",
      "Saved subtitles for Negro y Azul subtitles in Season 2\n",
      "Saved subtitles for Over subtitles in Season 2\n",
      "Saved subtitles for Peekaboo subtitles in Season 2\n",
      "Saved subtitles for Phoenix subtitles in Season 2\n",
      "Saved subtitles for Seven Thirty-Seven subtitles in Season 2\n",
      "Saved subtitles for Abiqui%C3%BA subtitles in Season 3\n",
      "Saved subtitles for Caballo Sin Nombre subtitles in Season 3\n",
      "Saved subtitles for Fly subtitles in Season 3\n",
      "Saved subtitles for Full Measure subtitles in Season 3\n",
      "Saved subtitles for Green Light subtitles in Season 3\n",
      "Saved subtitles for Half Measures subtitles in Season 3\n",
      "Saved subtitles for I See You subtitles in Season 3\n",
      "Saved subtitles for I.F.T. subtitles in Season 3\n",
      "Saved subtitles for Kafkaesque subtitles in Season 3\n",
      "Saved subtitles for M%C3%A1s subtitles in Season 3\n",
      "Saved subtitles for No M%C3%A1s subtitles in Season 3\n",
      "Saved subtitles for One Minute subtitles in Season 3\n",
      "Saved subtitles for Sunset subtitles in Season 3\n",
      "Saved subtitles for Box Cutter subtitles in Season 4\n",
      "Saved subtitles for Bug subtitles in Season 4\n",
      "Saved subtitles for Bullet Points subtitles in Season 4\n",
      "Saved subtitles for Cornered subtitles in Season 4\n",
      "Saved subtitles for Crawl Space subtitles in Season 4\n",
      "Saved subtitles for End Times subtitles in Season 4\n",
      "Saved subtitles for Face Off subtitles in Season 4\n",
      "Saved subtitles for Hermanos subtitles in Season 4\n",
      "Saved subtitles for Open House subtitles in Season 4\n",
      "Saved subtitles for Problem Dog subtitles in Season 4\n",
      "Saved subtitles for Salud subtitles in Season 4\n",
      "Saved subtitles for Shotgun subtitles in Season 4\n",
      "Saved subtitles for Thirty-Eight Snub subtitles in Season 4\n",
      "Saved subtitles for Buyout subtitles in Season 5A\n",
      "Saved subtitles for Dead Freight subtitles in Season 5A\n",
      "Saved subtitles for Fifty-One subtitles in Season 5A\n",
      "Saved subtitles for Gliding Over All subtitles in Season 5A\n",
      "Saved subtitles for Hazard Pay subtitles in Season 5A\n",
      "Saved subtitles for Live Free or Die subtitles in Season 5A\n",
      "Saved subtitles for Madrigal subtitles in Season 5A\n",
      "Saved subtitles for Say My Name subtitles in Season 5A\n",
      "Saved subtitles for Blood Money subtitles in Season 5B\n",
      "Saved subtitles for Buried subtitles in Season 5B\n",
      "Saved subtitles for Confessions subtitles in Season 5B\n",
      "Saved subtitles for Felina subtitles in Season 5B\n",
      "Saved subtitles for Granite State subtitles in Season 5B\n",
      "Saved subtitles for Ozymandias subtitles in Season 5B\n",
      "Saved subtitles for Rabid Dog subtitles in Season 5B\n",
      "Saved subtitles for To%27hajiilee subtitles in Season 5B\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Base URL\n",
    "base_url = \"https://breakingbad.fandom.com/wiki/Category:Breaking_Bad_Subtitles\"\n",
    "\n",
    "def get_season_links(base_url, target_seasons):\n",
    "    response = requests.get(base_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    season_links = []\n",
    "    \n",
    "    for link in soup.select('a.category-page__member-link'):\n",
    "        for season in target_seasons:\n",
    "            if f\"Season_{season}\" in link['href']:\n",
    "                season_links.append(\"https://breakingbad.fandom.com\" + link['href'])\n",
    "    return season_links\n",
    "\n",
    "def get_episode_links(season_url):\n",
    "    response = requests.get(season_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    episode_links = []\n",
    "    for link in soup.select('a.category-page__member-link'):\n",
    "        episode_links.append(\"https://breakingbad.fandom.com\" + link['href'])\n",
    "    return episode_links\n",
    "\n",
    "def get_subtitles(episode_url):\n",
    "    response = requests.get(episode_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    subtitle_pre = soup.find(\"pre\")\n",
    "    if subtitle_pre:\n",
    "        subtitles = subtitle_pre.get_text(strip=True)\n",
    "        return subtitles\n",
    "    return \"\"\n",
    "\n",
    "def save_subtitles(episode_name, subtitles, season):\n",
    "    # Handle \"5A\" and \"5B\" cases\n",
    "    season_folder = f\"Season_{season}\"\n",
    "    os.makedirs(f\"subtitles/{season_folder}\", exist_ok=True)\n",
    "    file_path = f\"subtitles/{season_folder}/{episode_name}.txt\"\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(subtitles)\n",
    "\n",
    "def scrape_and_save_subtitles():\n",
    "    target_seasons = [1, 2, 3, 4, \"5A\", \"5B\"]\n",
    "    season_links = get_season_links(base_url, target_seasons)\n",
    "    \n",
    "    for season_url in season_links:\n",
    "        # Extract season from URL\n",
    "        season = None\n",
    "        for s in target_seasons:\n",
    "            if f\"Season_{s}\" in season_url:\n",
    "                season = s\n",
    "                break\n",
    "                \n",
    "        if season:\n",
    "            episode_links = get_episode_links(season_url)\n",
    "            for episode_url in episode_links:\n",
    "                subtitles = get_subtitles(episode_url)\n",
    "                episode_name = episode_url.split(\"/\")[-1].replace(\"_\", \" \")\n",
    "                save_subtitles(episode_name, subtitles, season)\n",
    "                print(f\"Saved subtitles for {episode_name} in Season {season}\")\n",
    "                time.sleep(0.2)  # Be respectful to the server\n",
    "\n",
    "# Run the scraper and saver\n",
    "scrape_and_save_subtitles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of Extraction Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"Extract relationships between companies and technologies from the given text. Focus only on relationships where a company owns, develops, or implements a specific technology. Provide output in this JSON format:\n",
    "{\n",
    " \"edges\": [\n",
    " {\"from\": \"Company Name\", \"to\": \"Technology Name\", \"type\": \"relationship_type\", \"tech_type\": \"Technology Category\"}\n",
    " ]\n",
    "}\n",
    "The \"type\" field should be \"owns\", \"develops\", or \"implements\".\n",
    "The \"tech_type\" field should categorize the technology into one of these types:\n",
    "1. Customer Service and Support AI\n",
    "2. AI Infrastructure and Operations\n",
    "3. Robotics and Autonomous Systems\n",
    "4. Construction and Manufacturing AI\n",
    "5. Healthcare AI Applications\n",
    "6. Business Process and Workflow Automation\n",
    "7. Extended Reality (AR/VR) and Immersive Technologies\n",
    "8. AI in Mobile and Imaging\n",
    "9. AI Audio and Video Generation\n",
    "10. Search and Information Retrieval AI\n",
    "11. Financial Technology (FinTech) and Financial AI\n",
    "12. Smart Home and IoT AI\n",
    "13. E-Commerce AI Solutions\n",
    "14. Cybersecurity AI Solutions\n",
    "15. Recruitment and Human Resources (HR) AI\n",
    "16. Media and Content Personalization AI\n",
    "17. Data Analytics and Business Intelligence\n",
    "18. Software Development and DevOps AI Tools\n",
    "19. Generative and Multimodal AI\n",
    "20. Educational and Training AI\n",
    "\n",
    "Ensure a valid JSON object with an 'edges' array, even if empty. English output only.\n",
    "\n",
    "Examples based on the input articles:\n",
    "1. {\"from\": \"Google\", \"to\": \"AI-powered conversational chatbot\", \"type\": \"develops\", \"tech_type\": \"Customer Service and Support AI\"}\n",
    "2. {\"from\": \"OpenAI\", \"to\": \"ChatGPT desktop app for macOS\", \"type\": \"develops\", \"tech_type\": \"AI Infrastructure and Operations\"}\n",
    "3. {\"from\": \"YouTube\", \"to\": \"AI chatbot for Premium subscribers\", \"type\": \"implements\", \"tech_type\": \"Customer Service and Support AI\"}\n",
    "4. {\"from\": \"Apple\", \"to\": \"AI training curriculum for Developer Academy\", \"type\": \"develops\", \"tech_type\": \"Educational and Training AI\"}\n",
    "5. {\"from\": \"Adobe\", \"to\": \"Firefly AI for text-to-video generation\", \"type\": \"develops\", \"tech_type\": \"AI Audio and Video Generation\"}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relationships(article):\n",
    "    prompt = f\"\"\"\n",
    "    Extract key relationships between companies and technologies from this text:\n",
    "    Title: {article['title']}\n",
    "    Text: {article['text']}\n",
    "    Focus on relationships where a company owns, develops, or implements a specific technology.\n",
    "    Categorize each technology according to the tech_type categories provided.\n",
    "    \"\"\"\n",
    "    response = ollama.chat(\n",
    "        model='qwen2.5',\n",
    "        messages=[\n",
    "            {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "            {'role': 'user', 'content': prompt},\n",
    "        ],\n",
    "        format='json',\n",
    "        options={\"temperature\":0.1}\n",
    "    )\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Classification using..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
